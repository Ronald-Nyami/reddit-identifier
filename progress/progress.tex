\documentclass{article}

\usepackage{url}
\usepackage{cite}
\usepackage{graphicx}


\author{Adam Novak}
\title{Identifying Internet Users by Writing Style: Progress Report}

\begin{document}

\maketitle

\section{Introduction}
With the exception of communities like Facebook, people generally use pseudonymous identities when interacting online. While providing some measure of identity protection, using a pseudonym is not the same as being anonymous. In addition to more traditional attacks on pseudonymity, such as convincing or coercing websites or service providers to disclose users' identities, it has recently been shown that author identification based on writing style (also known as ``stylometry'') could practically be used to identify blog authors on an Internet-wide scale \cite{narayanan2012feasibility}. For my project, I aim to apply similar techniques to attempt to re-identify comments on Reddit, popular Internet discussion site \cite{reddit2012reddit}.

Stylometry has recently been shown to be a potentially practical method of identifying psuedonymous users at scale \cite{narayanan2012feasibility}. On a data set of posts from public 100,000~blogs, researchers were able to identify the correct blog for groups of three test posts about 20\% of the time, and, when taking into account algorithm confidence, could confidently identify the blogs of about 50\% of post sets, with about 80\% accuracy on guesses made \cite{narayanan2012feasibility}. Given that choosing a blog randomly would be, in the 100,000-blog case, only 0.001\% accurate, these results are drastically better than chance. Moreover, being able to identify half of all psuedonymous users 80\% correctly severely compromises the identity protections afforded by pseudonymity.

The goal of my project is to replicate these results, to the extent that I can, on user comments on the Internet discussion site Reddit.com \cite{reddit2012reddit}. While, in the context of blogs, pseudonymity is only of significant importance to the limited fraction of bloggers who blog about particularly controversial material, on Reddit pseudonymity is much more highly valued. This is partly due to the culture of the site's community, and partly due to users' practice of creating and maintaining so-called ``throwaway'' accounts which they use to discuss highly controversial and embarrassing material. If it were possible to identify users reliably based on writing style in Reddit comments, the pseudonymity of the site's users would be compromised, and the community would be profoundly affected.

\section{Methodology and Plans}
The data set for this project consists of 237,889~comments from 514~users from the comments sections of Reddit posts from the ``front page'' ``subreddits'' of Reddit.com, downloaded in several batches over the course of two days in November 2012 \cite{reddit2012reddit}. The total data set size is 46.8~megabytes, stored as Python tuples of (user name, comment, creation time) serialized using the \texttt{pickle} module. Comment text is in the form of Unicode strings with inline formatting, known as ``Reddit Makrkdown''. The data was downloaded using the Python Reddit API Wrapper (\texttt{praw}) Python module \cite{boe2012python}. At several different times, for each post in the site's front-page post listing, the list of users who commented on that post was obtained. For each unique user, up to 1000 of their comments, starting with the most recent, were downloaded. This process was continued until a certain number of users with over a certain number of comments was reached, or until an error occurred and no more data could be obtained. The data from several such retrieval sessions was then concatenated into the final data set, which is known to contain duplicate comments by at least one user.

When analyzing the data, I only consider users with comment histories of 100 or more comments. This limits the total number of users analyzed to 369. I also remove duplicate comment tuples.

Currently, bag-of-words and character-frequency feature extraction methods have been implemented to extract features from comment text. I plan to analyze bag-of-words feature extraction in my final report, because it is a very common feature extraction method, but I plan to extend the character-frequency feature extraction method to include other ``content-free'' feature types, such as frequency of words by length or by arrangement of capital letters, comment length, frequency of function words, and (ideally) frequency of part-of-speech relationships, as described in \cite{narayanan2012feasibility}. I may also add feature types related specifically to frequency of Reddit Markdown structures in comment formatting, since it seems likely that different users might use the available formatting options in different ways.

Currently, I have implemented Naive Bayes classification based on the bag-of-words features, and Decision Tree classification based on the character frequency features. I would like to use each feature set with each classifier, but I have only written the code to use the classifiers built into the Natural Language Took Kit (NLTK), which is the natural language analysis module I am using \cite{nltk2012classify, perk2010text}. These classifiers are not particularly well-engineered: the Naive Bayes classifier only appears to support discrete feature values, while the decision tree classifier does not support features that have values for some examples but not for others (one might expect it to provide the Pythonic value of \texttt{None} for missing features). Because of the shortcomings of these classifiers, I intend to replace them with the classifiers offered by scikit-learn, for which NLTK offers a convenient adapter class, \texttt{SklearnClassifier} \cite{nltk2012classify}. I expect the scikit-learn classifiers to be better-engineered, and I also expect there to be a wider variety of classifiers available, since NLTK is primarily a text-processing package, and its machine-learning components are understandably limited. Moreover, I plan to remove the decision tree classifier altogether, since it has several hyperparameters that I have no good ideas for how to tune, and it does not seem intuitively like the right algorithm for the job. Furthermore, decision trees are not mentioned as being used in \cite{narayanan2012feasibility}.

I have not yet attempted to tune any algorithm's hyperparameters. Currently, I split my data into only a training and a test set. In order to tune hyperparameters, I will need to split into a training, test, and validation set. I plan to use automated selection of the best combination of numeric hyperparameters for each algorithm I test, from lists of manually chosen values for each parameter. Consequently, I plan to limit myself to algorithms that have relatively few hyperparameters, such as k-nearest-neighbor, SVMs, or Naive Bayes. Coincidentally, those three algorithms have been found to be well-suited to stylometry-based author identification \cite{narayanan2012feasibility}.

For my experiment, I will test all combinations of feature extraction methods and learning algorithms, and find the pairing which performs the best on the final validation set. For classifiers that support it, I will look at the rank of the correct author in the ranked list of probable authors, and produce an alternative evaluation of classifier accuracy based on that rank. Depending on the comment activity of Reddit users, and on time constraints, I may even be able to download new comments by my test users and attempt to identify those. Although confidence estimation (and the ability for the classifier to abstain from predicting when uncertain) have been found to raise accuracy substantially for this type of problem, I do not intend to include these in my experiment \cite{narayanan2012feasibility}.

The amount of computing time that my experiment will require is difficult to estimate. When run on a substantially smaller data set of 20 users, the two classifiers that I have implemented take around 75~seconds total to be trained and have their accuracy evaluated, on my machine. When attempting to analyze the full data set, I have found that my system is quickly limited by its amount of physical memory (4~gigabytes). Consequently, I expect the runtime of my experiment to be determined primarily by how much memory I can make available to it. Currently, I plan to run my actual experiment on CampusRocks, the SoE cluster, where the machines have 16~gigabytes of physical memory each. However, CampusRocks does not currently have the technical setup necessary to run my analysis program: its default version of Python is too old. I am working on getting in contact with the CampusRocks maintainers to find out how I can run jobs on the cluster on a newer version of Python. I will also look into parallelizing my experiment, as each learning algorithm I want to test, for each set of hyperparameter settings, can run in parallel with the others. I would ideally like to parallelize using MPI across machines, but this may be technically difficult, especially taking into account the limitations imposed on my by the configuration of the cluster. If I am not able to use CampusRocks effectively for my project, I could try running my analysis on one of the SoE ``dance'' servers, but this option would limit my scalability in terms of parallelization over different learning algorithms and sets of hyperparameters. I may be forced to restrict my analysis to a subset of the data that I have obtained, depending on the computing resources I can marshal, in order to obtain an acceptable runtime. I intend to either increase the computing resources I allocate or decrease my data set size so as to obtain a total wall-clock runtime of approximately 2 hours for my experimental script.


\section{Progress and Problems}

So far, I have retrieved more than enough data to analyze (and am now limited by memory). I have also implemented the most basic feature extraction and classification (bag-of-words and Naive Bayes) that one would apply to this sort of classification problem. Initial results are encouraging: on my testing data set of 20 users, Naive Bayes with bag-of-words features identifies the correct user for a comment about 15\% of the time, versus an expected 5\% of the time when picking randomly among 20 equally likely classes. (I have not done a null model analysis to compare Naive Bayes classification to guessing randomly weighted according to the number of comments that each user has in the dataset.) The decision tree classifier on the character frequency feature set is much less accurate: not noticeably better than chance. I ascribe this deficiency to not having implemented most of the content-free features that have been shown to be useful for this type of problem, and potentially also to the algorithm possibly being a bad choice for the problem \cite{narayanan2012feasibility}.

The biggest obstacle I have encountered so far is under-engineering (and under-documentation) of the NLTK built-in classifiers. They do not all appear to work on both categorical and numeric features, and some seem to require every feature to have a defined value for each example, which would make my initial design choice of having a pure function to map directly from a comment's contents to a set of features unworkable for all but the simplest feature extraction methods. Consequently, I am planning to switch exclusively to classifiers in the scikit.learn module, through NLTK's interface to that package. I expect these classifiers to be better-engineered and better-documented. Additionally, a much wider range of classifiers is available.

The second-biggest problem I have encountered is getting my program running on the high-performance computing resources available to me. I have been trying to use CampusRocks, but its software configuration seems highly outdated (Python 2.4 is older than many elementary-school children) and frankly perverse. As I have said above, I have attempted to get in contact with the SoE support staff about my issues with CampusRocks, but I may be forced to run my analysis on the non-clustered SoE servers, where the software configuration is more reasonable.

In conclusion, despite the problems I have encountered, I have already managed to demonstrate better-than-chance identification of pseudonymous Reddit users by stylometry, and I believe that I can still perform an experiment substantially identical to that which I have proposed.

\section{Bibliography}

\nocite{*}
\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}
